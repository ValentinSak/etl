services:
  postgres:
    image: postgres:latest
    container_name: postgres_db
    restart: always
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow_db
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  postgres_etl:
    image: postgres:latest
    container_name: postgres_etl_db
    restart: always
    environment:
      POSTGRES_USER: etl_user
      POSTGRES_PASSWORD: etl_password
      POSTGRES_DB: etl_db
    ports:
      - "5433:5432"
    volumes:
      - ./etl_db/postgres_etl_data:/var/lib/postgresql/data
      - ./etl_db/procedures:/docker-entrypoint-initdb.d
      - ./import_files:/data

  airflow:
    image: apache/airflow:2.7.3
    container_name: airflow
    restart: always
    depends_on:
      - postgres
      - postgres_etl
    environment:
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: airflow
      _AIRFLOW_WWW_USER_PASSWORD: airflow
      DEFAULT_PASSWORD: admin
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow_db
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      ETL_DB_CONN_ID: etl_db_con
    ports:
      - "8080:8080"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    entrypoint: >
      /bin/bash -c "airflow db migrate &&
      sleep 10 &&
      airflow connections get 'etl_db_con' ||
      airflow connections add 'etl_db_con' 
      --conn-type 'Postgres' 
      --conn-host 'postgres_etl' 
      --conn-schema 'etl_db' 
      --conn-login 'etl_user' 
      --conn-password 'etl_password' 
      --conn-port '5432' &&
      airflow webserver & airflow scheduler"

volumes:
  postgres_data:
  postgres_etl_data:
  airflow_dags:
  airflow_logs:
  airflow_plugins: